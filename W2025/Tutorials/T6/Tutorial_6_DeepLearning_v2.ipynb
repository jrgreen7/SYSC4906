{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jrgreen7/SYSC4906/blob/master/W2025/Tutorials/T6/Tutorial_6_DeepLearning_v2.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "# Tutorial 6 - Deep Learning\n",
    "\n",
    "**Course:** SYS 4415 - Introduction to Machine Learning\n",
    "\n",
    "**Semester:** Winter 2025\n",
    "\n",
    "**Adapted by:** [Kevin Dick](https://kevindick.ai/), [Igor Bogdanov](igorbogdanov@cmail.carleton.ca)\n",
    "\n",
    "**Adapted from:** [Keras v. TensorFlow Tutorial](https://www.pyimagesearch.com/2018/10/08/keras-vs-tensorflow-which-one-is-better-and-which-one-should-i-learn/)\n",
    "\n",
    "---\n",
    "\n",
    "In this two-part notebook, we will demonstrate the creation of a Convolutional Neural Network classifier using both Keras and Pytorch.\n",
    "\n",
    "**Part 0: Introduction to the Keras API**\n",
    "\n",
    "**Part I: CNN for CIFAR-10 Classification**\n",
    "1. load a benchmark computer vision dataset (CIFAR-10)\n",
    "2. create a VGGNet-like architecture\n",
    "3. train, evaluate, and test the model on the CIFAR-10 benchmark\n",
    "\n",
    "---\n",
    "# Part 0: Introduction to the Keras API\n",
    "\n",
    "## Keras: The Python Deep Learning API\n",
    "\n",
    "Keras is the **high-level API of TensorFlow 2.0**, meant to be an approachable and highly-productive interface for solving machine learning problems, with a focus on modern deep learning. It **provides essential abstractions** and building blocks for developing and shipping machine learning solutions with high iteration velocity.\n",
    "\n",
    "## The Convolutional Neural Network (CNN)\n",
    "\n",
    "In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, **most commonly applied to analyzing visual imagery**. They are also known as **shift invariant** or **space invariant artificial neural networks**, based on their shared-weights architecture and translation invariance characteristics.\n",
    "\n",
    "In this tutorial, we will implement a VGG-type CNN based on the work of the **V**isual **G**eometry **G**roup at the University of Oxford. Several pre-trained model architectures and learned weights are easily downloadable from the Keras library, such as the **VGG-16 model**.\n",
    "\n",
    "**Additional Reading**: [5 Popular CNN Architectures Clearly Explained and Visualized](https://towardsdatascience.com/5-most-well-known-cnn-architectures-visualized-af76f1f0065e/)\n",
    "\n",
    "---\n",
    "\n",
    "When first learning to create deep learning models, it is important to understand the role of each type of layer; familiarization of the **[Keras API documentation](https://keras.io/api/layers/))** is one of the best approaches!\n",
    "\n",
    "For convenience, three of the **[Core Layers](https://keras.io/api/layers/core_layers/)** covered in part I of this tutorial are replicated here:\n",
    "\n",
    "### Keras Layers: **[Input Object](https://keras.io/api/layers/core_layers/input/)**\n",
    "\n",
    "```python\n",
    "tf.keras.Input(\n",
    "    shape=None,     # A shape tuple (integers), not including the batch size. For instance, shape=(32,) indicates that the expected input will be batches of 32-dimensional vectors.\n",
    "    batch_size=None,# optional static batch size (integer).\n",
    "    name=None,      # An optional name string for the layer. Should be unique in a model (do not reuse the same name twice).\n",
    "    dtype=None,     # The data type expected by the input, as a string (float32, float64, int32...)\n",
    "    sparse=False,   # A boolean specifying whether the placeholder to be created is sparse.\n",
    "    tensor=None,    # Optional existing tensor to wrap into the Input layer.\n",
    "    ragged=False,   # A boolean specifying whether the placeholder to be created is ragged.\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "### Keras Layers: **[Dense Layer](https://keras.io/api/layers/core_layers/dense/)**\n",
    "\n",
    "\n",
    "```python\n",
    "tf.keras.layers.Dense(\n",
    "    units,                              # Positive integer, dimensionality of the output space.\n",
    "    activation=None,                    # Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "    use_bias=True,                      # Boolean, whether the layer uses a bias vector.\n",
    "    kernel_initializer=\"glorot_uniform\",# Initializer for the kernel weights matrix.\n",
    "    bias_initializer=\"zeros\",           # Initializer for the bias vector.\n",
    "    kernel_regularizer=None,            # Regularizer function applied to the kernel weights matrix.\n",
    "    bias_regularizer=None,              # Regularizer function applied to the bias vector.\n",
    "    activity_regularizer=None,          # Regularizer function applied to the output of the layer (its \"activation\").\n",
    "    kernel_constraint=None,             # Constraint function applied to the kernel weights matrix.\n",
    "    bias_constraint=None,               # Constraint function applied to the bias vector.\n",
    "    **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "### Keras Layers: **[Activation Layer](https://keras.io/api/layers/core_layers/activation/)**\n",
    "```python\n",
    "tf.keras.layers.Activation(\n",
    "  activation, # Activation function, such as tf.nn.relu, or string name of built-in activation function, such as \"relu\"\n",
    "  **kwargs\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Part I: Building a VGG16 CNN Classifer for the the CIFAR-10 Dataset\n",
    "\n",
    "- VGG16 developed by the VGG (Visual Geometry Group, University of Oxford) for the ILSVRC-2014 competition.\n",
    "- Published at: [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)\n",
    "- Note that VGG16 is available (pre-trained) through Keras.applications\n",
    "\n",
    "```python\n",
    "from keras.applications import VGG16\n",
    "VGG_model = VGG16()\n",
    "VGG_model.summary()\n",
    "```\n",
    "    \n",
    "- VGGNet-like architectures are characterized by:\n",
    "    1. Using only 3Ã—3 convolutional layers stacked on top of each other in increasing depth\n",
    "    2. Reducing volume size by max pooling\n",
    "    3. Fully-connected layers at the end of the network prior to a softmax classifier\n",
    "\n",
    "\n",
    "### The CIFAR-10 Dataset\n",
    "\n",
    "**CIFAR** = \"Canadian Institute For Advanced Research\"\n",
    "\n",
    "A **benchmark dataset** is a reference database against which learning models are evaluated and performance assessed to determine whether or not a given approach is superior over another.\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images.\n",
    "\n",
    "The classes are: [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "The dataset is divided into **five training batches and one test batch**, each with 10,000 images. The **test batch contains exactly 1,000 randomly selected images from each class**. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5,000 images from each class.\n",
    "\n",
    "Published at: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "---\n",
    "\n",
    "### Hardware Acceleration: Enabling the Collab GPU\n",
    "\n",
    "To accelerate the training phase and inference phase of deep learning models, Google Colab provides GPUs that greatly accelerate model development.\n",
    "\n",
    "For example, using CPU alone, each epoch of the CNN would require ~450 seconds, whereas with GPU, each only requires ~7s.\n",
    "\n",
    "To verify whether GPUs are enabled:  `Runtime` $\\rightarrow$ `Change runtime type` $\\rightarrow$ `Hardware accelerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qT5h_hOcPNsr",
    "outputId": "f226a5c2-6c8f-4217-b32e-b831d422bb9f"
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer # For 1-hot encoding of 10 class labels\n",
    "from keras.datasets import cifar10  \t\t\t\t\t\t # Built-in dataset of images.\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "!pip install torchsummary \n",
    "!pip install pydot\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "!pip install --upgrade --no-deps torchviz\n",
    "from torchviz import make_dot\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Create directories if they don't exist to save the artifacts and model checkpoints\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints_keras\", exist_ok=True)\n",
    "os.makedirs(\"checkpoints_torch\", exist_ok=True)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check if CUDA/MPS is available\n",
    "device = (\n",
    "    torch.device(\"cuda\")\n",
    "    if torch.cuda.is_available()\n",
    "    else (\n",
    "        torch.device(\"mps\")\n",
    "        if torch.backends.mps.is_available()\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Chosen device:{device}\")\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"GPU Name:\", tf.test.gpu_device_name())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaC0vSNCz1zm"
   },
   "source": [
    "### Define the \"MiniVGGnetKeras\" Class and \"MiniVGGNetTorch\"\n",
    "\n",
    "These classes will define the structure of the VGG deep network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSnX1H7l3Dpy"
   },
   "outputs": [],
   "source": [
    "# Define a class to represent the VGG-type network. Instantiate using build()\n",
    "class MiniVGGNetKeras:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes):\n",
    "\t\t# initialize the input shape and channel dimension, assuming\n",
    "\t\t# TensorFlow/channels-last ordering\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# define the model input (layer)\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\n",
    "  \t# first (CONV => RELU) * 2 => POOL layer set\n",
    "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(inputs) # padding=same --> output same size as input by padding by filter/2 rows/cols of zeros\n",
    "\t\tx = Activation(\"relu\")(x) \t\t\t\t\t\t\t\t\t\t # Add a rectified linear unit activation function to each conv layer\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)  \t\t\t # Use batch normalization. We will talk about this later\n",
    "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x) \t\t\t\t\t# MaxPooling uased after the first 2 conv layers. Reduces size of subsequent layers by down-sampling\n",
    "\t\tx = Dropout(0.25)(x)  \t\t\t\t\t\t\t\t\t\t\t\t\t# Dropout used to promote generalization. We'll talk about this layer type later in the course...\n",
    "\n",
    "\t\t# second (CONV => RELU) * 2 => POOL layer set\n",
    "\t\tx = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "\n",
    "    # first (and only) set of FC => RELU layers\n",
    "\t\tx = Flatten()(x)  # Flatten the 2D maxPooling2D layer to feed into a fully-connected layer\n",
    "\t\tx = Dense(512)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization()(x)\n",
    "\t\tx = Dropout(0.5)(x)\n",
    "\n",
    "\t\t# Softmax classifier layer for arriving at normalized prediction scores for multi-class problem.\n",
    "    # Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0.\n",
    "    # Only makes sense when each image is expected to represent a single class (not a dog & a cat simultaneously)\n",
    "\t\tx = Dense(classes)(x)\n",
    "\t\tx = Activation(\"softmax\")(x)\n",
    "\n",
    "\t\t# create the model and give it a unique name\n",
    "\t\tmodel = Model(inputs, x, name=\"minivggnet_keras\")\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model\n",
    "\n",
    "\n",
    "\n",
    "class MiniVGGNetTorch(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MiniVGGNetTorch, self).__init__()\n",
    "\n",
    "        # First (CONV => RELU) * 2 => POOL layer set\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "        # Second (CONV => RELU) * 2 => POOL layer set\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(0.25),\n",
    "        )\n",
    "\n",
    "        # FC => RELU layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.fc(x)\n",
    "        return F.softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBlf-SNB0a3T"
   },
   "source": [
    "# Download and prepare the CIFAR-10 dataset \n",
    "We are downloading both for Keras and PyTorch to keep everything separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNIPZtw30ejP",
    "outputId": "916dff0b-0c29-485a-c8e4-2f8cd8377057"
   },
   "outputs": [],
   "source": [
    "# load the training and testing data, then scale it into the range [0, 1]\n",
    "print(\"[INFO] loading CIFAR-10 data...\")\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype(\"float\") / 255.0\n",
    "testX  = testX.astype(\"float\") / 255.0\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)\n",
    "\n",
    "# initialize the label names for the CIFAR-10 dataset\n",
    "label_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n",
    "\t\"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "\n",
    "def load_cifar10_data(batch_size=32):\n",
    "    \"\"\"\n",
    "    Load and preprocess CIFAR-10 dataset.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Number of samples per batch\n",
    "\n",
    "    Returns:\n",
    "        trainloader: DataLoader for training data\n",
    "        testloader: DataLoader for test data\n",
    "        classes: Tuple of class names\n",
    "    \"\"\"\n",
    "    print(\"[INFO] loading CIFAR-10 data...\")\n",
    "\n",
    "    # Define the transformation pipeline\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),  # Convert PIL Image to tensor and scale to [0,1]\n",
    "            transforms.Normalize(\n",
    "                (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "            ),  # Normalize with mean/std\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Load training data\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    # Load test data\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    # Define class names\n",
    "    classes = (\n",
    "        \"airplane\",\n",
    "        \"automobile\",\n",
    "        \"bird\",\n",
    "        \"cat\",\n",
    "        \"deer\",\n",
    "        \"dog\",\n",
    "        \"frog\",\n",
    "        \"horse\",\n",
    "        \"ship\",\n",
    "        \"truck\",\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader, classes\n",
    "\n",
    "trainloader, testloader, classes = load_cifar10_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ME1RYqOJz-9O"
   },
   "source": [
    "# Build VGG16 models using the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "o1Znz2OktTPY",
    "outputId": "5592135e-b02b-4d8b-ecb0-fbb8bc69b5d9"
   },
   "outputs": [],
   "source": [
    "# initialize the initial learning rate, total number of epochs to\n",
    "# train for, and batch size\n",
    "BATCH_SIZE = 32\n",
    "INIT_LEARNING_RATE = 0.01\n",
    "EPOCHS = 10 # Each epoch takes 450s using CPU and only ~30s using GPU (Runtime->Change runtime->EnableGPU)\n",
    "\n",
    "# initialize the optimizer and model\n",
    "print(\"[INFO] compiling model...\")\n",
    "\n",
    "opt = SGD(learning_rate=INIT_LEARNING_RATE, decay=INIT_LEARNING_RATE / EPOCHS)\n",
    "model_keras = MiniVGGNetKeras.build(width=32, height=32, depth=3, classes=len(label_names))\n",
    "model_keras.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    os.path.join(\"checkpoints_keras\", \"minivggnet_keras_cifar10_epoch_{epoch:02d}.keras\"),\n",
    "    monitor='val_loss',\n",
    "    verbose=1,\n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    save_freq='epoch'\n",
    ")\n",
    "\n",
    "print(\"[INFO] saving model visualization...\")\n",
    "# Save as PNG\n",
    "plot_model(model_keras,\n",
    "    to_file=os.path.join('images', 'model_architecture_keras.png'),\n",
    "    show_shapes=True, show_layer_names=True, dpi=64,  # Lower DPI for smaller file size\n",
    "    rankdir=\"TB\",  # Left to right layout like PyTorch\n",
    "    expand_nested=True,  # Show internal operations\n",
    "    show_layer_activations=True )\n",
    "\n",
    "# For display in notebook\n",
    "display(Image(os.path.join('images', 'model_architecture_keras.png')))\n",
    "\n",
    "print(model_keras.summary())\n",
    "\n",
    "def initialize_model(num_classes, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    Initialize the CNN model, loss function, and optimizer.\n",
    "\n",
    "    Args:\n",
    "        num_classes: Number of output classes\n",
    "        learning_rate: Initial learning rate\n",
    "        epochs: Total number of epochs for learning rate decay\n",
    "\n",
    "    Returns:\n",
    "        model: Initialized model\n",
    "        loss_function: Loss function\n",
    "        optimizer: Optimizer\n",
    "    \"\"\"\n",
    "    print(\"[INFO] compiling model...\")\n",
    "\n",
    "    # Initialize model and move to appropriate device\n",
    "    print(f\"Chosen device:{device}\")\n",
    "    model = MiniVGGNetTorch(num_classes=num_classes).to(device)\n",
    "    \n",
    "\n",
    "    # Define loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Initialize optimizer with learning rate decay\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        momentum=0.9,\n",
    "        weight_decay=learning_rate / epochs,  # This is equivalent to Keras' decay\n",
    "    )\n",
    "\n",
    "    # Print model summary using torchsummary if available\n",
    "    try:\n",
    "        print(\"\\nModel Summary:\")\n",
    "        if str(device) == \"mps\":\n",
    "            # Temporarily move model to CPU for summary\n",
    "            model_cpu = model.to('cpu')\n",
    "            summary(model_cpu, input_size=(3, 32, 32), device='cpu')\n",
    "            # Move model back to MPS\n",
    "            model = model.to(device)\n",
    "        else:\n",
    "            summary(model, input_size=(3, 32, 32), device=str(device))\n",
    "    except ImportError:\n",
    "        # Fallback to basic model print\n",
    "        print(\"\\nModel Architecture:\")\n",
    "        print(model)\n",
    "        # Print parameter count\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "    # Optional: Visualize model architecture using torchviz\n",
    "    try:\n",
    "        model.eval()  # Set model to evaluation mode before visualization\n",
    "        sample_input = torch.randn(32, 3, 32, 32).to(device)  # Use same batch_size as training\n",
    "        dot = make_dot(model(sample_input), params=dict(model.named_parameters()))\n",
    "        display(dot)\n",
    "        print(\"[INFO] saving model visualization...\")\n",
    "        dot.render(os.path.join(\"images\", \"model_architecture_torch\"), format=\"png\", cleanup=True)\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"torchviz not installed. Skip model visualization.\")\n",
    "\n",
    "    return model, loss_function, optimizer\n",
    "\n",
    "model_torch, loss_function_torch, optimizer_torch = initialize_model(num_classes=len(classes), learning_rate=INIT_LEARNING_RATE, epochs=EPOCHS)\n",
    "\n",
    "print(\"[INFO] Models have been initialized...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ME1RYqOJz-9O"
   },
   "source": [
    "# Train VGG16 models using the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kKoxYc5f8_9y",
    "outputId": "694cbf87-18fd-4474-8dc1-9b173697928b",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# train the network\n",
    "\n",
    "print(\"Training data shapes:\")\n",
    "print(f\"trainX shape: {trainX.shape}\")\n",
    "print(f\"trainY shape: {trainY.shape}\")\n",
    "print(f\"testX shape: {testX.shape}\")\n",
    "print(f\"testY shape: {testY.shape}\")\n",
    "\n",
    "print(\"[INFO] training network for {0:d} epochs...\".format(EPOCHS))\n",
    "H = model_keras.fit(trainX, \n",
    "                    trainY, \n",
    "                    validation_data=(testX, testY),\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[checkpoint],  # Add checkpoint callback\n",
    "                    verbose=1)\n",
    "\n",
    "print(\"[INFO] saving final model...\")\n",
    "model_keras.save(os.path.join(\"checkpoints_keras\", \"minivggnet_keras_cifar10_final.keras\"))\n",
    "\n",
    "# In Keras, training is handled by the high-level model.fit() method,\n",
    "# while PyTorch requires an explicit training loop.\n",
    "\n",
    "def train_cnn(epochs, batch_size, learning_rate, model, criterion, optimizer, trainloader, testloader, classes):\n",
    "    print(\"[INFO] training network...\")\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Training progress bar\n",
    "        train_pbar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for i, data in enumerate(train_pbar):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{running_loss/(i+1):.4f}',\n",
    "                'acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_loss = running_loss / len(trainloader)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # Validation progress bar\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        val_pbar = tqdm(testloader, desc='Validation')\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in val_pbar:\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                # Update validation progress bar\n",
    "                val_pbar.set_postfix({\n",
    "                    'val_loss': f'{val_loss/(i+1):.4f}',\n",
    "                    'val_acc': f'{100.*correct/total:.2f}%'\n",
    "                })\n",
    "\n",
    "        val_loss = val_loss / len(testloader)\n",
    "        val_acc = 100 * correct / total\n",
    "\n",
    "        # Store history\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        # Save checkpoint after each epoch\n",
    "        print(f\"\\n[INFO] saving checkpoint for epoch {epoch+1}...\")\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"history\": history,\n",
    "            },\n",
    "             os.path.join(\"checkpoints_torch\", f\"minivggnet_torch_cifar10_epoch_{epoch+1}.pth\")\n",
    "        )\n",
    "\n",
    "    # Save final model\n",
    "    print(\"[INFO] saving final model...\")\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epochs,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"history\": history,\n",
    "        },\n",
    "        os.path.join(\"checkpoints_torch\", \"minivggnet_torch_cifar10_final.pth\")\n",
    "    )\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "model_torch_trained, history_torch = train_cnn(EPOCHS, BATCH_SIZE, INIT_LEARNING_RATE, model_torch, loss_function_torch, optimizer_torch, trainloader, testloader, classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ME1RYqOJz-9O"
   },
   "source": [
    "# Compare VGG16 models performance over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "id": "82Lvht-2w8RU",
    "outputId": "d12bb2a8-1c6c-483b-eba6-84e469a9d9b8",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))\n",
    "fig.suptitle(\"Training Loss and Accuracy on Dataset\")\n",
    "\n",
    "# Keras plots (top row)\n",
    "ax1.plot(np.arange(0, EPOCHS), H.history[\"loss\"], label=\"train_loss\")\n",
    "ax1.plot(np.arange(0, EPOCHS), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "ax1.set_title('Keras Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(np.arange(0, EPOCHS), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "ax2.plot(np.arange(0, EPOCHS), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "ax2.set_title('Keras Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "# PyTorch plots (bottom row)\n",
    "ax3.plot(np.arange(0, EPOCHS), history_torch[\"train_loss\"], label=\"train_loss\")\n",
    "ax3.plot(np.arange(0, EPOCHS), history_torch[\"val_loss\"], label=\"val_loss\")\n",
    "ax3.set_title('PyTorch Loss')\n",
    "ax3.legend()\n",
    "\n",
    "ax4.plot(np.arange(0, EPOCHS), history_torch[\"train_acc\"], label=\"train_acc\")\n",
    "ax4.plot(np.arange(0, EPOCHS), history_torch[\"val_acc\"], label=\"val_acc\")\n",
    "ax4.set_title('PyTorch Accuracy')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join('images', 'training_history_comparison.png'))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fytqSXt01JXU"
   },
   "source": [
    "# Test the trained VGG16 network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z3E5lq9E1T9o",
    "outputId": "4ebba76a-a9b4-4260-a6bb-3e0faf8c5d1e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate Keras model\n",
    "print(\"\\nKeras Model Evaluation:\")\n",
    "predictions_keras = model_keras.predict(testX, batch_size=32)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "    predictions_keras.argmax(axis=1), target_names=label_names))\n",
    "\n",
    "# Evaluate PyTorch model\n",
    "print(\"\\nPyTorch Model Evaluation:\")\n",
    "model_torch.eval()  # Set to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_data = torch.FloatTensor(testX).permute(0, 3, 1, 2).to(device)  # Convert to PyTorch format\n",
    "    predictions_torch = model_torch(test_data)\n",
    "    predictions_torch = predictions_torch.cpu().numpy()\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "    predictions_torch.argmax(axis=1), target_names=label_names))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iScLdHhHZAYV"
   },
   "source": [
    "The **[f1-score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)** gives you the **harmonic mean of precision and recall**. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.\n",
    "\n",
    "The **[support](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)** is the **number of samples of the true response** that lie in **that class**."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
