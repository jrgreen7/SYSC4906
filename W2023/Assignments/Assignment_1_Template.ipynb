{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlsCc0RI5ZF4IfP4ClNrK2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\"\"\"\n","GENERAL NOTE: Please print out numbers (i.e., accuracies, plots, etc.) so they are visible without me having to run your colab notebook. Use python's print() function.\n","\"\"\"\n","\n","# import everything you might need\n","\n","import torch\n","import math\n","import matplotlib.pyplot as plt\n","from sklearn.linear_model import LogisticRegression, LinearRegression\n","from sklearn.metrics import accuracy_score, mean_squared_error\n","import pandas as pd\n","from xgboost import XGBClassifier\n","import random\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np"],"metadata":{"id":"6DFH29u4vrvS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**QUESTION 1**"],"metadata":{"id":"oK3G_6TexTi7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDbfBJXcvWV1"},"outputs":[],"source":["\"\"\"\n","Q1: Calculate the gradient of the f(x, y, z) = 7x2z – 2xy3 + 5z at (-1, -2, 3). What does this vector represent?\n","\n","YOUR ANSWER HERE\n","\"\"\""]},{"cell_type":"markdown","source":["**QUESTION 2**"],"metadata":{"id":"7p8galnwxL_L"}},{"cell_type":"code","source":["\"\"\"\n","Q2a: Use pandas to load train.csv, val.csv, and test.csv into three separate dataframes. Then, create three scatter plots of the train set with features A, B, and C on the x-axes and Y on the y-axes.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"PlE8_uM_vmxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q2b: If you were using A, B, and C to predict Y (i.e., using linear regression), how well do you think these features would perform? Do you think nonlinear functions would fit the data better? Why or why not?\n","\n","YOUR ANSWER HERE\n","\"\"\""],"metadata":{"id":"nJ4PZt77wHbT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q2c: The data is already normalized, how can you tell?\n","\n","YOUR CODE AND ANSWER BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"sIhPuIPDwkw9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q2d: For each of the three input features, compute its square-root to create 3 additional features. Repeat for squaring the feature values to create another 3 features.\n","You now have 9 total input features: A, B, C, sqrt(A), sqrt(B), sqrt(C), A2, B2, C2. \n","Do this for all three data splits (train, val, and test). You can think of these as polynomial kernels used in SVMs.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"-jJ1cxmvwu90"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**QUESTION 3**"],"metadata":{"id":"lxXg_ih4xdDU"}},{"cell_type":"code","source":["\"\"\"\n","Q3a: Train a linear regression model on the train set using only the 3 original input features.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"dhri8_3yxfFz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q3b: What are the mean absolute errors of the train set, val set, and test sets using this model? Did the model overfit the train set?\n","\n","YOUR CODE AND ANSWER BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"Xbqp56oCxl2s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q3c: What are the values of the 4 learned parameters (3 weights and 1 bias)? Do these values make sense given your scatter plots in Q2a?\n","\n","YOUR CODE AND ANSWER BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"0nQYH-U1xxZr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q3d: Train another linear regression model using all 9 input features.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"RFCzKCFqxz-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q3e: What are the mean absolute errors of each dataset, using this new model?\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"qBF4i2JFx4Dj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q3f: How many learned parameters are there for this model, and what are their values?\n","\n","YOUR CODE AND ANSWER BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"ylWEBJTcx8Bk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q3g: Which model performs better? How can you explain the difference in performance?\n","\n","YOUR ANSWER HERE\n","\"\"\""],"metadata":{"id":"rAT4ED6LyADA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**QUESTION 4**"],"metadata":{"id":"c1VYI6iayJS8"}},{"cell_type":"code","source":["\"\"\"\n","Q4a: Use scikit-learn to train a logistic regression model on the train set using only the 3 original features. Again, just use the default hyper-parameters.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"ZNpJivbuyLND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q4b: What are the overall accuracies of the train, val, and test sets? Use scikit-learn’s accuracy_score metric.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"l2aYUirgyRGk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q4c: Train another logistic regression model on the train set using all 9 features. If it doesn’t converge, set max_iter to 5,000.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"NJw0iqWvyUsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q4d: Repeat b, but with your new logistic regression model.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"V1TDc_aKyYBU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q4e: XGBoost is pre-installed on Google Colab. One hyper-parameter is the depth of the tree (called max_depth). Find the optimal value of max_depth, all other hyper-parameters can be ignored for this question.\n","That is, your model should look like: model = XGBClassifier(max_depth=L).fit(X,y). \n","Hint: the test set should not be used to search for hyper-parameters. Train these models using only the 3 original features.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"fLo2-iTsyccG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q4f: Repeat Step b, but with your new XGBoost model (with the max_depth chosen from e).\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"WvqNZeBpyjjs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q4g: Of your 3 models—two logistic regression models and one XGBoost model—which performs best on the test set? Why do you think this is the case?\n","\n","YOUR ANSWER HERE\n","\"\"\""],"metadata":{"id":"0a9vbr2Jymq0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**QUESTION 5**"],"metadata":{"id":"oKzItLTcytY0"}},{"cell_type":"code","source":["\"\"\"\n","Q5a: Using all 9 input features, we want to train a linear model (i.e., torch.nn.Linear) to predict the class label.\n","This is a multi-class classification problem, which of the following loss functions, available in PyTorch, best suits our task? Explain briefly.\n","torch.nn.CrossEntropyLoss()\n","torch.nn.MSELoss()\n","torch.nn.BCELoss()\n","\n","\n","YOUR ANSWER HERE\n","\"\"\""],"metadata":{"id":"8dQQk_Layvgk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q5b: Your model will output 10 numbers, one for each class.\n","These are called logits. Do you need to compute the softmax across these 10 logits before applying your loss function (from a), or does the loss function compute the softmax for you?\n","\n","YOUR ANSWER HERE\n","\"\"\""],"metadata":{"id":"te2Fvb5Ky1Xk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q5c: Using the loss function from a, train a linear model using PyTorch and stochastic gradient descent (SGD).\n","Use a batch size of 1, a learning rate of 0.3 and train for 200 epochs. You can update your model weights “manually” or use an optimizer.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"Q11HwYosy5wM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q5d: Use the trained model to make predictions on the val and test sets. Calculate the overall accuracy of both sets.\n","\n","YOUR CODE BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"ffXSX4EnzAEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Q5e: Repeat parts c and d three times. You should get different results each run. There are two main reasons for this. What are they?\n","\n","YOUR CODE AND ANSWER BELOW, IN THIS CELL\n","\"\"\""],"metadata":{"id":"7VLYU9U6zEhk"},"execution_count":null,"outputs":[]}]}